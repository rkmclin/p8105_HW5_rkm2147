---
title: "Homework 5"
author: "Ronae McLin"
date: "11/11/2020"
output: github_document
---



```{r}
library(tidyverse)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d

scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1


Importing data

```{r}
homicide_df = read_csv(file = "./homicide-data.csv")
```

**Raw data explanation**

The raw homicide data frame contains the variables: `r names(homicide_df)`. There are a total of `r nrow(homicide_df)` contained within. Upon first glance it appears that we will need to tidy the variable concerning the reported data of the incident that occurred. Data collected for this frame was by The Washington Post,  it gathered information on homicides in 50 large U.S. cities


Create city_state variable & resolved variable
```{r}
tidy_homicide_df = 
homicide_df %>% 
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved",
    ))
```



```{r}
homicide_summary_df = 
  tidy_homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  )
```


Prop test for Baltimore, MD

```{r}
prop.test(
  homicide_summary_df %>% 
    filter(city_state == "Baltimore_MD") %>% pull(hom_unsolved) 
  
 , homicide_summary_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_total)) %>% 
  broom::tidy()
```



Running a prop test for each city in the data frame
```{r}
results_df = 
  homicide_summary_df %>% 
  mutate(
    prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)
```


A plot that shows the estimates and CIs for each city
```{r}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```



# Problem 2

Start with a dataframe containing all file names; the list.files function will help


```{r}
path_df = 
  tibble(
    path = list.files("data")
  ) %>% 
  mutate(
    path = str_c("data/", path),
    data = map(path, ~read_csv(.x))
    )  
 
```


Tidy the result; manipulate file names to include control arm and subject ID, make sure weekly observations are “tidy”, and do any other tidying that’s necessary 

```{r}
tidy_df =
path_df %>% 
 mutate(
path = str_replace(path, "data/con", "control"),
path = str_replace(path, "data/exp", "experiment"),
path = str_replace(path, ".csv", "")
) %>% 
   unnest(data) %>% 
   separate(
   path, into = c("treatment","id")
 ) %>% 
  pivot_longer( 
    week_1:week_8,
    names_to = "week",
    values_to = "value"
  ) %>% 
  mutate(
    treatment = as.factor(treatment),
    id = as.factor(id),
    week = as.factor(week)
  )
```


make a spaghetti plot showing observations on each subject over time, and comment on differences between groups.

need to figure out how to compare control vs treatment
```{r}
tidy_df %>% 
 
  ggplot(aes( x = week, y = value, group = id, color = id)) + geom_line() +  facet_grid(rows = vars(treatment)) 
```

*** switch the position of the facet, to better compare the values***
# problem 3



x∼Normal[μ,σ]

For each dataset, save μ̂
 and the p-value arising from a test of H:μ=0
 using α=0.05
. Hint: to obtain the estimate and p-value, use broom::tidy to clean the output of t.test.
```{r}
sim_ms = function(n = 30, mu, sigma = 5) {
  
  sim_data = tibble(
    x = rnorm(n = n, mean = mu, sd = sigma)
  )
  
  sim_data %>% 
    summarize(
      mu = mean(x),
      sigma = sd(x))
      
      sim_data %>% 
     t.test() %>% 
    broom::tidy() %>% 
        select(estimate, p.value)
  
}
```

Run for μ=0
```{r}
zero_output = 
  rerun(50, sim_ms(30, 0, 5)) %>% 
  bind_rows() %>% 
  select(
    estimate, p.value
  )
  
```







Re running for   μ={0,1,2,3,4,5,6}
```{r}
# don't believe this works
mu_list = 
  list(
    "mu_0"  = 0, 
    "mu_1"  = 1, 
    "mu_2" = 2, 
    "mu_3" = 3,  
    "mu_4" = 4, 
    "mu_5" = 5,
    "mu_6" = 6
    )

six_output = vector("list", length = 7)

for (i in 1:7) {
  six_output[[i]] = rerun(5, sim_ms(mu = mu_list[[i]])) %>% bind_rows() %>%  select(estimate, p.value)
}
```

trying maria's way
```{r}
new_six = 
  tibble(
    mu = c(0, 1, 2, 3, 4, 5, 6)) %>% 
      mutate(
        output_list = map(.x = mu, ~rerun(5000, sim_ms(mu = .x))),
        output_df = map(output_list, bind_rows)
      ) %>% 
      unnest(output_df) %>% 
      select(-output_list)
  
```



Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ (1-6)
 on the x axis. Describe the association between effect size and power.
 
 
```{r}
# i dont think this works...
graph_data =
bind_rows(six_output) %>%
mutate(
test_results = case_when(
p.value < .05 ~ "reject",
p.value >= .05 ~ "fail to reject"
))  %>% 
  summarize(
    total_test = n(),
    reject_test = sum(test_results == "reject")) %>% 
  mutate(
    power = (reject_test / total_test))
     
  
```
 
```{r}
rejection_plot = 
new_six %>% group_by(mu) %>% 
  count(p.value < .05) %>% 
  mutate(
    power = n/sum(n)
  ) %>% janitor::clean_names() %>% 
  filter(p_value_0_05 == TRUE) %>% 
  ggplot(aes(x = mu, y = power)) + geom_point() + geom_smooth() + labs(
    
    title = "The association between effect size and power",
      x = "True value of Mu",
    y = "Power"
  )
```
 
 The association between effect size and power is visualized in the rejection_plot. We can see that as the true value of mu for the sample increases, or in this case deviates from the null value of 0, the effect size increases.  This increase in the effect size results in the increase of the power for the study.  Power is (1- Beta), the probability of avoiding a type II error. Even at a considerably small effect size, when the true value of mu is very close to the null mu value, we see that the power is just above 0.0 and is not exactly 0.  I understand this is due to the random error and chance associated with studies that were simulated.  
 